[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Priya’s AI Research Reading Corner",
    "section": "",
    "text": "Supervised Pretraining Can Learn In-Context Reinforcement Learning\n\n\n\n\n\n\n\nai research\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nPriya Shanmugasundaram\n\n\n\n\n\n\n  \n\n\n\n\nLarge Language Models as Optimizers\n\n\n\n\n\n\n\nai research\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nPriya Shanmugasundaram\n\n\n\n\n\n\n  \n\n\n\n\nLarge Language Models are Semi-Parameteric RL Agents\n\n\n\n\n\n\n\nai research\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nPriya Shanmugasundaram\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Large Language Models are Semi-Parameteric RL Agents",
    "section": "",
    "text": "Hello Everyone! Welcome to my blog, today we’ll be going through the NeurIPS 2023 paper, “Large Langauge Models are Semi-Parametric RL Agents”.\nKey Points in Abstract:\n\nevolvable LLM-based agent called REMEMBERER is proposed, possesses long-term experience memory that can be exploited across different task goals.\nproposed approach apparently outperforms LLM agents with fixed examples and transient amount of memory.\nauthors introduce RLEM (RL with Experience Memory) - to update the memory\nthe method learns from experiences of both success and failure and evolves its capability by avoiding finetuning as memory is used to learn\nshow that its superior on two RL taks compared to SOTA\n\nKey Takeways from Intro:\n\nbased on psychology theory that states that humans use episodic memory of experiences from past episodes to make decisions, by repeating actions that caused success and avoiding actions that resulted in failure, extend this framework to seq decision making in LLMs.\nExisting work - 1) finetuning the parameters using RL - quite costly and computationally expensive 2) Algorithm distillation - in-context RL by embedding the RL training traj into the input prompt of a pre-trained decision transformer. - cannot embed whole experience due to input length\n\nThus RLEM - updates the experience memory rather than modifiying the model parameters - updates experience memory through analogical RL training so it can self-evolve\nREFLEXION - LLM with short-term working memory - authors introduce long-term experience memory as short term working memory is tied to a specific task goal and the stored memory cannot be used for different goals.\nInner Monologue, Corrective Re-prompting, DEPS - take advantage of immediate failure feedback only once?\nRLEM + LLM = REMEMBERER - utilize the experiences selectively based on the current state to optimize the decision\nsemi-parametric RL agent - evolve its ability through interaction experiences however without finetuning parameters\nExperimental setup\n\nRL Task sets with promising performance on LLM agents - web shop and wikihow\ntrained on a dew tasks and tested on some other tasks to check the inter-useability of task data and experiences.\n\nRLEM Pipeline:\n\nLLM agent observes \\(o_t\\) at time \\(t\\) from the environment and then \\(o_t\\) is used to retrieve several related experiences from the RLEM module according to some similarity functions, which are represented as \\(O_x\\), \\(a_x\\) and corresponding state-action value estimates \\(Q_x\\), where \\(x\\) is the subset of retrieved experiences.\nbased on that and the \\(r_{t-1}\\) LLM selects \\(a_t\\) which will be executed in the environment and the resulted reward \\(r_t\\) received as feedback.\nthe tuple containing the experiences (\\(o_t\\), \\(a_t\\), \\(r_t\\), \\(o_{t+1}\\)) are stored into the experience memory\nthe experiences are assumed to be a group of external parameters of the LLM based agents\noff policy learning - task info g, and tuple from last step - to estimate the Q value using bellman optimality equation where unrecorded actions for the task are assigned a Q value of 0 using Q Learning\nadapting the training memory for dynamic exemplars for few shot in-context Learning, where the exemplars are chosen based on similarity where two kinds of similarity are considered, task similarity and observation similarity between two observations \\(o_t\\) and \\(o_i\\)\n\n\\[\ns_i = \\lambda f_g(g, g_i) + (1 - \\lambda)f_o(o_t, o_i)\n\\]\nwhere \\(m\\) records with the highest similarity are retrieved for the exemplars. For the input part they show task information, past observations, past actions, and interaction feedback and in the output where encouraged and discouraged actions are shown based on the q values.\nDetails on Experiments:\nWebshop and WikiHow are conducted on the models OpenAI API of GPT3.5Turbo and text-davinci-003\nWebshop - instructed to browse the site and shop for target goods. score between 0 and 1 will be rated after shopping by assessing the correspondence between product and instruction.\nwebpage representation and a list of available actions — &gt; LLM —&gt; products to shop for\nno immediate rewards only the last 5 actions serve as procedure feedback\ninspired by cot and react - LLM is prompted to predict a reason for its decision – observation similarity based on web page representation into four patterns\ntask similiarity sentence similarity transformer \\(f_g\\)\nWikiHow - follow the instructions and navigate to the required page\nintermediate rewards are available, the screen is represented in an HTML element sequence, last 4 performed actions and the last reward are given as feedback\ntask representation, screen representation and step instruction –&gt; LLMs –&gt; print the HTML representation of the operated element\n\\(f_g\\) task similarity is computed from step instructions and \\(f_o\\) is computed based on the length of the longest common sequence of HTML elements in the screen representation\nThey present ablation analysis of full model with variants like w/o bootstrap, w/o random and w/o discouraged actions and so on and look at avg reward and avg success rates for the two task sets\nThey compare performance with LLM only method, ReAct and REMEMBERER for different training sets and different initial exemplar sets and see consistent improvements across different settings"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Priya Shanmugasundaram",
    "section": "",
    "text": "Helloo! I am Priya, an MS student in Computer Engineering at Virginia Tech, advised by Prof. Ming Jin. My research interests include Reinforcement Learning, Large Language Models, Optimization and AI safety. I am currently a Graduate Research Assistant in the ROLE Lab at Virginia Tech, where I work on inducing moral reasoning in AI agents to enable alignment and safety."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/blog1.html",
    "href": "posts/post-with-code/blog1.html",
    "title": "Large Language Models as Optimizers",
    "section": "",
    "text": "Hello everyone! Welcome to my blog, today we will be going through the paper “Large Language Models as Optimizers” from Google Deepmind.\nKey Points in the Abstract:\n\noptimization task is described in natural language and they leverage LLMs as optimizers.\nthey propose an Optimization by Prompting technique where at each step, the LLM generates new solutions from the prompt that contains prev generated solutions with their values, new solutions are evaluated and add to the prompt for the next step\nshow OPRO on LR and Traveling Salesman problem and also do prompt optimization where they want to find prompt that maximizes task accuracy\nobtain optimized prompts that improve accuracy by 8%, 50% on GSM8K and Big-bench Hard tasks compared to human prompts\n\nKey Points in the Introduction:\n\ninstead of formally defining the optimization problem and deriving the update update step with a solver, the method uses LLMs where the optimization problem is described in natural language and the solution obtained in the previous step is provided along with the prompt to the next step\nPrompt Optimization: want to optimize the prompt to maximize training accuracy, as the prompt can impact the performance of the model significantly. They assume that they have a training set available to compute the training accuracy as the objective balie for optimization\nmeta-prompt - 1) previously generated prompts with the corresponding accuracy 2) optimization problem description where several exempars are randomly selected from the training set\nstarting with prompts that have low accuracies, the LLM serves as an optimizer in generating prompts that increase the accuracy\n\nOPRO - LLM as the Optimizer\n\nWith every step, the LLM generates candidate solutions to the optimzation task by looking at the optimization problem description and previously evaluated solutions in the meta-Prompt\nthe new solutions are evaluated and added to the prompt for next steps, process terminate when max_steps are reached or optimization scores are constant\nthe LLM that evaluates the objective function is called the scorer LLM and the LLM for optimization as the optimizer LLMs\nMeta-Prompt Design - 1) Optimization Problem Description: text description of the optimization problem including the objective function and the solution constraints; 2) Optimization Trajectory: instructing the LLM to leveraging the optimization trajectory for generating new solutions - where trajectory has past solutions with optimization score in ascending order\nThey consider two optimization problems as example, 1) Linear Regression and 2) TSP for continuous and discrete problems respectively. They explore different (w,b) pairs compared to the w_true and b_true and evaluate the performance of their method in identifying close (w,b) pairs for different models and the number of steps to convergence as well.\nThey evaluate similarly for TSP setting as well, where they compare their method to existing heuristics like Nearest Neighbor and Farthest Insertion.\nFor their prompt optimization module, they evaluate performance of their prompts on GSM8K and BB-Hard tasks for different settings, A_begin, A_end, Q_begin and Q_end which denote the location where the prompt is inserted across different models.\nThey also perform ablation studies analyze the meta-prompt design wrt transferability, semantic similarity, number of few-shots, initial instruction, order of the prompts etc."
  }
]